{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Machine Learning approach for Malware Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy\n",
    "import pickle\n",
    "import pefile\n",
    "import sklearn.ensemble as ek\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn import cross_validation, tree, linear_model\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import tree\n",
    "from sklearn import linear_model\n",
    "import joblib as j\n",
    "\n",
    "#from sklearn.externals import joblib\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the initial dataset delimited by | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv('data.csv',sep='|', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of malicious files vs Legitimate files in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.groupby(dataset['legitimate']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping columns like Name of the file, MD5 (message digest) and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dataset.drop(['Name','md5','legitimate'],axis=1).values\n",
    "y = dataset['legitimate'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ExtraTreesClassifier\n",
    "ExtraTreesClassifier fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extratrees = ek.ExtraTreesClassifier().fit(X,y)\n",
    "model = SelectFromModel(extratrees, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "nbfeatures = X_new.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTreesClassifier helps in selecting the required features useful for classifying a file as either Malicious or Legitimate\n",
    "\n",
    "14 features are identified as required by ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nbfeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Cross Validation\n",
    "Cross validation is applied to divide the dataset into random train and test subsets.\n",
    "test_size = 0.2 represent the proportion of the dataset to include in the test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y ,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = []\n",
    "index = numpy.argsort(extratrees.feature_importances_)[::-1][:nbfeatures]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features identified by ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in range(nbfeatures):\n",
    "    print(\"%d. feature %s (%f)\" % (f + 1, dataset.columns[2+index[f]], extratrees.feature_importances_[index[f]]))\n",
    "    features.append(dataset.columns[2+f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the below Machine Learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = { \"DecisionTree\":tree.DecisionTreeClassifier(max_depth=10),\n",
    "         \"RandomForest\":ek.RandomForestClassifier(n_estimators=100),\n",
    "         \"Adaboost\":ek.AdaBoostClassifier(n_estimators=50),\n",
    "         \"GradientBoosting\":ek.GradientBoostingClassifier(n_estimators=50),\n",
    "         \"GNB\":GaussianNB(),\n",
    "         \"LinearRegression\":LinearRegression()   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training each of the model with the X_train and testing with X_test.\n",
    "The model with best accuracy will be ranked as winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for algo in model:\n",
    "    clf = model[algo]\n",
    "    clf.fit(X_train,y_train)\n",
    "    score = clf.score(X_test,y_test)\n",
    "    print (\"%s : %s \" %(algo, score))\n",
    "    results[algo] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "winner = max(results, key=results.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "j.dump(model[winner],'classifier/classifier.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "open('classifier/features.pkl', 'wb').write(pickle.dumps(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the False positive and negative on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DecisionTreeClassifier' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [127]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[39m=\u001b[39m model[winner]\n\u001b[1;32m      2\u001b[0m op \u001b[39m=\u001b[39mclf\u001b[39m.\u001b[39mfit(X_train,y_train)\n\u001b[0;32m----> 3\u001b[0m res \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(\u001b[39mbytearray\u001b[39;49m(op))\n\u001b[1;32m      4\u001b[0m mt \u001b[39m=\u001b[39m confusion_matrix(y, res)\n\u001b[1;32m      5\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFalse positive rate : \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%%\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m ((mt[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39msum\u001b[39m(mt[\u001b[39m0\u001b[39m])))\u001b[39m*\u001b[39m\u001b[39m100\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DecisionTreeClassifier' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "clf = model[winner]\n",
    "op =clf.fit(X_train,y_train)\n",
    "res = clf.predict(bytearray(op))\n",
    "mt = confusion_matrix(y, res)\n",
    "print(\"False positive rate : %f %%\" % ((mt[0][1] / float(sum(mt[0])))*100))\n",
    "print('False negative rate : %f %%' % ( (mt[1][0] / float(sum(mt[1]))*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load classifier\n",
    "clf = joblib.load('classifier/classifier.pkl')\n",
    "#load features\n",
    "features = pickle.loads(open(os.path.join('classifier/features.pkl'),'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing with unseen file\n",
    "Given any unseen test file, it's required to extract the characteristics of the given file.  \n",
    "\n",
    "In order to test the model on an unseen file, it's required to extract the characteristics of the given file. Python's pefile.PE library is used to construct and build the feature vector and a ML model is used to predict the class for the given file based on the already trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '-f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36m<cell line: 185>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m clf \u001b[39m=\u001b[39m joblib\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mclassifier/classifier.pkl\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    188\u001b[0m features \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mloads(\u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39mclassifier/features.pkl\u001b[39m\u001b[39m'\u001b[39m),\u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mread())\n\u001b[0;32m--> 189\u001b[0m data \u001b[39m=\u001b[39m extract_infos(sys\u001b[39m.\u001b[39;49margv[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m    190\u001b[0m pe_features \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x:data[x], features)\n\u001b[1;32m    192\u001b[0m res\u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict([pe_features])[\u001b[39m0\u001b[39m]    \n",
      "Input \u001b[0;32mIn [110]\u001b[0m, in \u001b[0;36mextract_infos\u001b[0;34m(fpath)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_infos\u001b[39m(fpath):\n\u001b[1;32m     79\u001b[0m     res \u001b[39m=\u001b[39m {}\n\u001b[0;32m---> 80\u001b[0m     pe \u001b[39m=\u001b[39m pefile\u001b[39m.\u001b[39;49mPE(fpath)\n\u001b[1;32m     81\u001b[0m     res[\u001b[39m'\u001b[39m\u001b[39mMachine\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39mFILE_HEADER\u001b[39m.\u001b[39mMachine\n\u001b[1;32m     82\u001b[0m     res[\u001b[39m'\u001b[39m\u001b[39mSizeOfOptionalHeader\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m pe\u001b[39m.\u001b[39mFILE_HEADER\u001b[39m.\u001b[39mSizeOfOptionalHeader\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pefile.py:2743\u001b[0m, in \u001b[0;36mPE.__init__\u001b[0;34m(self, name, data, fast_load, max_symbol_exports, max_repeated_symbol)\u001b[0m\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2740'>2741</a>\u001b[0m fast_load \u001b[39m=\u001b[39m fast_load \u001b[39mor\u001b[39;00m \u001b[39mglobals\u001b[39m()[\u001b[39m\"\u001b[39m\u001b[39mfast_load\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2741'>2742</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2742'>2743</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__parse__(name, data, fast_load)\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2743'>2744</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2744'>2745</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pefile.py:2790\u001b[0m, in \u001b[0;36mPE.__parse__\u001b[0;34m(self, fname, data, fast_load)\u001b[0m\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2782'>2783</a>\u001b[0m \u001b[39m\"\"\"Parse a Portable Executable file.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2783'>2784</a>\u001b[0m \n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2784'>2785</a>\u001b[0m \u001b[39mLoads a PE file, parsing all its structures and making them available\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2785'>2786</a>\u001b[0m \u001b[39mthrough the instance's attributes.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2786'>2787</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2788'>2789</a>\u001b[0m \u001b[39mif\u001b[39;00m fname \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2789'>2790</a>\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(fname)\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2790'>2791</a>\u001b[0m     \u001b[39mif\u001b[39;00m stat\u001b[39m.\u001b[39mst_size \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   <a href='file:///home/karthik/.local/lib/python3.8/site-packages/pefile.py?line=2791'>2792</a>\u001b[0m         \u001b[39mraise\u001b[39;00m PEFormatError(\u001b[39m\"\u001b[39m\u001b[39mThe file is empty\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '-f'"
     ]
    }
   ],
   "source": [
    "# %load malware_test.py\n",
    "\"\"\"\n",
    "this file extracts the required information of a given file using the library PE \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pefile\n",
    "import os\n",
    "import array\n",
    "import math\n",
    "import pickle\n",
    "#from sk-learn.externals import joblib\n",
    "import sys\n",
    "import argparse\n",
    "import sklearn\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "def get_entropy(data):\n",
    "    if len(data) == 0:\n",
    "\t    return 0\n",
    "    occurences = array.array('L', [0]*256)\n",
    "    for x in data:\n",
    "  \t    occurences[x if isinstance(x, int) else ord(x)] += 1\n",
    "\n",
    "    entropy = 0\n",
    "    for x in occurences:\n",
    "\t    if x:\n",
    "\t        p_x = float(x) / len(data)\n",
    "\t        entropy -= p_x*math.log(p_x, 2)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def get_resource(pe):\n",
    "    resources = []\n",
    "    \n",
    "    if hasattr(pe, 'DIRECTORY_ENTRY_RESOURCE'):\n",
    "        try:\n",
    "            for resource_type in pe.DIRECTORY_ENTRY_RESOURCE.entries:\n",
    "                if hasattr(resource_type, 'directory'):\n",
    "                    for resource_id in resource_type.directory.entries:\n",
    "                        if hasattr(resource_id,'directory'):\n",
    "                            for resource_lang in resource_id.directory.entries:\n",
    "                                data = pe.get_data(resource_lang.data.struct.OffsetToData, resource_lang.data.struct.Size)\n",
    "                                size = resource_lang.data.struct.Size\n",
    "                                entropy = get_entropy(data)\n",
    "                                resources.append([entropy,size])\n",
    "                                \n",
    "        except Exception as i:\n",
    "            return resources\n",
    "    return resources\n",
    "\n",
    "def get_version_info(pe):\n",
    "    \"\"\"Return version infos\"\"\"\n",
    "    res = {}\n",
    "    for fileinfo in pe.FileInfo:\n",
    "        if fileinfo.Key == 'StringFileInfo':\n",
    "            for st in fileinfo.StringTable:\n",
    "                for entry in st.entries.items():\n",
    "                    res[entry[0]] = entry[1]\n",
    "        if fileinfo.Key == 'VarFileInfo':\n",
    "            for var in fileinfo.Var:\n",
    "                res[var.entry.items()[0][0]] = var.entry.items()[0][1]\n",
    "    if hasattr(pe, 'VS_FIXEDFILEINFO'):\n",
    "          res['flags'] = pe.VS_FIXEDFILEINFO.FileFlags\n",
    "          res['os'] = pe.VS_FIXEDFILEINFO.FileOS\n",
    "          res['type'] = pe.VS_FIXEDFILEINFO.FileType\n",
    "          res['file_version'] = pe.VS_FIXEDFILEINFO.FileVersionLS\n",
    "          res['product_version'] = pe.VS_FIXEDFILEINFO.ProductVersionLS\n",
    "          res['signature'] = pe.VS_FIXEDFILEINFO.Signature\n",
    "          res['struct_version'] = pe.VS_FIXEDFILEINFO.StrucVersion\n",
    "    return res\n",
    "\n",
    "#extract the info for a given file\n",
    "def extract_infos(fpath):\n",
    "    \n",
    "    res = {}\n",
    "    pe = pefile.PE(fpath)\n",
    "    res['Machine'] = pe.FILE_HEADER.Machine\n",
    "    res['SizeOfOptionalHeader'] = pe.FILE_HEADER.SizeOfOptionalHeader\n",
    "    res['Characteristics'] = pe.FILE_HEADER.Characteristics\n",
    "    res['MajorLinkerVersion'] = pe.OPTIONAL_HEADER.MajorLinkerVersion\n",
    "    res['MinorLinkerVersion'] = pe.OPTIONAL_HEADER.MinorLinkerVersion\n",
    "    res['SizeOfCode'] = pe.OPTIONAL_HEADER.SizeOfCode\n",
    "    res['SizeOfInitializedData'] = pe.OPTIONAL_HEADER.SizeOfInitializedData\n",
    "    res['SizeOfUninitializedData'] = pe.OPTIONAL_HEADER.SizeOfUninitializedData\n",
    "    res['AddressOfEntryPoint'] = pe.OPTIONAL_HEADER.AddressOfEntryPoint\n",
    "    res['BaseOfCode'] = pe.OPTIONAL_HEADER.BaseOfCode\n",
    "    try:\n",
    "        res['BaseOfData'] = pe.OPTIONAL_HEADER.BaseOfData\n",
    "    except AttributeError:\n",
    "        res['BaseOfData'] = 0\n",
    "    res['ImageBase'] = pe.OPTIONAL_HEADER.ImageBase\n",
    "    res['SectionAlignment'] = pe.OPTIONAL_HEADER.SectionAlignment\n",
    "    res['FileAlignment'] = pe.OPTIONAL_HEADER.FileAlignment\n",
    "    res['MajorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MajorOperatingSystemVersion\n",
    "    res['MinorOperatingSystemVersion'] = pe.OPTIONAL_HEADER.MinorOperatingSystemVersion\n",
    "    res['MajorImageVersion'] = pe.OPTIONAL_HEADER.MajorImageVersion\n",
    "    res['MinorImageVersion'] = pe.OPTIONAL_HEADER.MinorImageVersion\n",
    "    res['MajorSubsystemVersion'] = pe.OPTIONAL_HEADER.MajorSubsystemVersion\n",
    "    res['MinorSubsystemVersion'] = pe.OPTIONAL_HEADER.MinorSubsystemVersion\n",
    "    res['SizeOfImage'] = pe.OPTIONAL_HEADER.SizeOfImage\n",
    "    res['SizeOfHeaders'] = pe.OPTIONAL_HEADER.SizeOfHeaders\n",
    "    res['CheckSum'] = pe.OPTIONAL_HEADER.CheckSum\n",
    "    res['Subsystem'] = pe.OPTIONAL_HEADER.Subsystem\n",
    "    res['DllCharacteristics'] = pe.OPTIONAL_HEADER.DllCharacteristics\n",
    "    res['SizeOfStackReserve'] = pe.OPTIONAL_HEADER.SizeOfStackReserve\n",
    "    res['SizeOfStackCommit'] = pe.OPTIONAL_HEADER.SizeOfStackCommit\n",
    "    res['SizeOfHeapReserve'] = pe.OPTIONAL_HEADER.SizeOfHeapReserve\n",
    "    res['SizeOfHeapCommit'] = pe.OPTIONAL_HEADER.SizeOfHeapCommit\n",
    "    res['LoaderFlags'] = pe.OPTIONAL_HEADER.LoaderFlags\n",
    "    res['NumberOfRvaAndSizes'] = pe.OPTIONAL_HEADER.NumberOfRvaAndSizes\n",
    "\n",
    "    # Sections\n",
    "    res['SectionsNb'] = len(pe.sections)\n",
    "    entropy = map(lambda x:x.get_entropy(), pe.sections)\n",
    "    res['SectionsMeanEntropy'] = sum(entropy)/float(len(entropy))\n",
    "    res['SectionsMinEntropy'] = min(entropy)\n",
    "    res['SectionsMaxEntropy'] = max(entropy)\n",
    "    raw_sizes = map(lambda x:x.SizeOfRawData, pe.sections)\n",
    "    res['SectionsMeanRawsize'] = sum(raw_sizes)/float(len(raw_sizes))\n",
    "    res['SectionsMinRawsize'] = min(raw_sizes)\n",
    "    res['SectionsMaxRawsize'] = max(raw_sizes)\n",
    "    virtual_sizes = map(lambda x:x.Misc_VirtualSize, pe.sections)\n",
    "    res['SectionsMeanVirtualsize'] = sum(virtual_sizes)/float(len(virtual_sizes))\n",
    "    res['SectionsMinVirtualsize'] = min(virtual_sizes)\n",
    "    res['SectionMaxVirtualsize'] = max(virtual_sizes)\n",
    "\n",
    "    #Imports\n",
    "    try:\n",
    "        res['ImportsNbDLL'] = len(pe.DIRECTORY_ENTRY_IMPORT)\n",
    "        imports = sum([x.imports for x in pe.DIRECTORY_ENTRY_IMPORT], [])\n",
    "        res['ImportsNb'] = len(imports)\n",
    "        res['ImportsNbOrdinal'] = len(filter(lambda x:x.name is None, imports))\n",
    "    except AttributeError:\n",
    "        res['ImportsNbDLL'] = 0\n",
    "        res['ImportsNb'] = 0\n",
    "        res['ImportsNbOrdinal'] = 0\n",
    "\n",
    "    #Exports\n",
    "    try:\n",
    "        res['ExportNb'] = len(pe.DIRECTORY_ENTRY_EXPORT.symbols)\n",
    "    except AttributeError:\n",
    "        # No export\n",
    "        res['ExportNb'] = 0\n",
    "    #Resources\n",
    "    resources= get_resources(pe)\n",
    "    res['ResourcesNb'] = len(resources)\n",
    "    if len(resources)> 0:\n",
    "        entropy = map(lambda x:x[0], resources)\n",
    "        res['ResourcesMeanEntropy'] = sum(entropy)/float(len(entropy))\n",
    "        res['ResourcesMinEntropy'] = min(entropy)\n",
    "        res['ResourcesMaxEntropy'] = max(entropy)\n",
    "        sizes = map(lambda x:x[1], resources)\n",
    "        res['ResourcesMeanSize'] = sum(sizes)/float(len(sizes))\n",
    "        res['ResourcesMinSize'] = min(sizes)\n",
    "        res['ResourcesMaxSize'] = max(sizes)\n",
    "    else:\n",
    "        res['ResourcesNb'] = 0\n",
    "        res['ResourcesMeanEntropy'] = 0\n",
    "        res['ResourcesMinEntropy'] = 0\n",
    "        res['ResourcesMaxEntropy'] = 0\n",
    "        res['ResourcesMeanSize'] = 0\n",
    "        res['ResourcesMinSize'] = 0\n",
    "        res['ResourcesMaxSize'] = 0\n",
    "\n",
    "    # Load configuration size\n",
    "    try:\n",
    "        res['LoadConfigurationSize'] = pe.DIRECTORY_ENTRY_LOAD_CONFIG.struct.Size\n",
    "    except AttributeError:\n",
    "        res['LoadConfigurationSize'] = 0\n",
    "\n",
    "\n",
    "    # Version configuration size\n",
    "    try:\n",
    "        version_infos = get_version_info(pe)\n",
    "        res['VersionInformationSize'] = len(version_infos.keys())\n",
    "    except AttributeError:\n",
    "        res['VersionInformationSize'] = 0\n",
    "    return res\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\t\n",
    "    clf = joblib.load('classifier/classifier.pkl')\n",
    "    features = pickle.loads(open(os.path.join('classifier/features.pkl'),'rb').read())\n",
    "    data = extract_infos(sys.argv[1])\n",
    "    pe_features = map(lambda x:data[x], features)\n",
    "\n",
    "    res= clf.predict([pe_features])[0]    \n",
    "    print ('The file %s is %s' % (os.path.basename(sys.argv[1]),['malicious', 'legitimate'][res]))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the program to test the file - Skype.exe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run malware_test.py \"/home/surajr/Downloads/Skype.exe\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test for the malicious file, an application has been downloaded from malwr.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run malware_test.py \"/home/surajr/Downloads/BCN12ui49823.exe\""
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
